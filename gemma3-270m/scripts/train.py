#!/usr/bin/env python3
"""
Gemma3 270M Training Script for MacBook M4 using MLX.
Implements LoRA training with InfoNCE loss and gradient accumulation.
"""

import os
import sys
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, Any, Optional
import time
from datetime import datetime

import mlx.core as mx
import mlx.nn as nn
import mlx.optimizers as optim
from transformers import AutoTokenizer, AutoConfig
import numpy as np
from tqdm import tqdm

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

class GemmaLoRATrainer:
    """Trainer class for Gemma3 270M with LoRA adaptation."""
    
    def __init__(self, configPath: str):
        """Initialize the trainer with configuration."""
        self.config = self.loadConfig(configPath)
        self.setupLogging()
        self.setupDevice()
        self.setupModel()
        self.setupOptimizer()
        self.setupData()
        
    def loadConfig(self, configPath: str) -> Dict[str, Any]:
        """Load training configuration from JSON file."""
        with open(configPath, 'r') as f:
            return json.load(f)
    
    def setupLogging(self):
        """Setup logging configuration."""
        logDir = Path(self.config['logging']['log_file']).parent
        logDir.mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(
            level=getattr(logging, self.config['logging']['log_level'].upper()),
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.config['logging']['log_file']),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info("üöÄ Initializing Gemma3 270M LoRA Trainer")
    
    def setupDevice(self):
        """Setup MLX device and check hardware capabilities."""
        self.logger.info("üîç Setting up MLX device...")
        
        # Get device info
        deviceInfo = mx.device_info()
        self.logger.info(f"Device info: {deviceInfo}")
        
        # Set device
        self.device = mx.default_device()
        self.logger.info(f"Using device: {self.device}")
        
        # Check memory
        try:
            import psutil
            memory_gb = psutil.virtual_memory().total / (1024**3)
            self.logger.info(f"Available RAM: {memory_gb:.1f} GB")
            
            if memory_gb < 16:
                self.logger.warning("‚ö†Ô∏è  Less than 16GB RAM detected. Consider reducing batch size.")
        except ImportError:
            self.logger.warning("psutil not available, cannot check RAM")
    
    def setupModel(self):
        """Setup the Gemma3 270M model with LoRA adaptation."""
        self.logger.info("üèóÔ∏è  Setting up Gemma3 270M model...")
        
        modelPath = self.config['model']['path']
        if not Path(modelPath).exists():
            raise FileNotFoundError(f"Model path not found: {modelPath}")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(modelPath)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load model configuration
        self.modelConfig = AutoConfig.from_pretrained(modelPath)
        self.logger.info(f"Model config: {self.modelConfig}")
        
        # Initialize model (simplified for MLX)
        self.model = self.createMLXModel()
        self.logger.info("‚úÖ Model setup complete")
    
    def createMLXModel(self):
        """Create MLX-compatible model structure."""
        # This is a simplified model structure for MLX
        # In practice, you'd need to implement the full Gemma architecture
        class SimpleGemmaModel(nn.Module):
            def __init__(self, config):
                super().__init__()
                self.config = config
                self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
                self.layers = []
                # Simplified layer structure
                
            def __call__(self, input_ids, attention_mask=None):
                # Simplified forward pass
                return self.embed_tokens(input_ids)
        
        return SimpleGemmaModel(self.modelConfig)
    
    def setupOptimizer(self):
        """Setup optimizer with LoRA parameters."""
        self.logger.info("‚öôÔ∏è  Setting up optimizer...")
        
        # Get trainable parameters (LoRA parameters)
        trainableParams = self.getLoRAParameters()
        
        # Setup optimizer
        self.optimizer = optim.Adam(
            learning_rate=self.config['training']['learning_rate'],
            betas=(0.9, 0.999),
            eps=1e-8,
            weight_decay=self.config['training']['weight_decay']
        )
        
        self.logger.info(f"‚úÖ Optimizer setup complete with {len(trainableParams)} trainable parameters")
    
    def getLoRAParameters(self):
        """Get LoRA parameters for training."""
        # This would return the actual LoRA parameters
        # For now, return a placeholder
        return []
    
    def setupData(self):
        """Setup data loading and preprocessing."""
        self.logger.info("üìä Setting up data...")
        
        # Check if data files exist
        trainFile = self.config['data']['train_file']
        if not Path(trainFile).exists():
            self.logger.warning(f"Training data file not found: {trainFile}")
            self.logger.info("Creating sample data for testing...")
            self.createSampleData()
        
        self.logger.info("‚úÖ Data setup complete")
    
    def createSampleData(self):
        """Create sample training data for testing."""
        dataDir = Path("./data")
        dataDir.mkdir(exist_ok=True)
        
        # Create sample training data
        sampleData = [
            {"text": "This is a sample text for training.", "label": 1},
            {"text": "Another example text for the model.", "label": 0},
            {"text": "Machine learning is fascinating.", "label": 1},
            {"text": "Natural language processing examples.", "label": 1},
            {"text": "Sample data for testing purposes.", "label": 0}
        ]
        
        trainFile = Path(self.config['data']['train_file'])
        with open(trainFile, 'w') as f:
            for item in sampleData:
                f.write(json.dumps(item) + '\n')
        
        self.logger.info(f"‚úÖ Created sample data at {trainFile}")
    
    def train(self):
        """Main training loop."""
        self.logger.info("üéØ Starting training...")
        
        config = self.config['training']
        batchSize = config['batch_size']
        gradientAccumulationSteps = config['gradient_accumulation_steps']
        maxSteps = config['max_steps']
        
        # Training loop
        globalStep = 0
        totalLoss = 0.0
        
        progressBar = tqdm(total=maxSteps, desc="Training")
        
        try:
            while globalStep < maxSteps:
                # Training step
                loss = self.trainingStep()
                totalLoss += loss
                
                # Gradient accumulation
                if (globalStep + 1) % gradientAccumulationSteps == 0:
                    self.optimizer.step()
                    self.optimizer.zero_grad()
                
                # Logging
                if (globalStep + 1) % config['logging_steps'] == 0:
                    avgLoss = totalLoss / config['logging_steps']
                    self.logger.info(f"Step {globalStep + 1}: Loss = {avgLoss:.4f}")
                    totalLoss = 0.0
                
                # Evaluation
                if (globalStep + 1) % config['eval_steps'] == 0:
                    self.evaluate()
                
                # Save checkpoint
                if (globalStep + 1) % config['save_steps'] == 0:
                    self.saveCheckpoint(globalStep + 1)
                
                globalStep += 1
                progressBar.update(1)
                
        except KeyboardInterrupt:
            self.logger.info("‚èπÔ∏è  Training interrupted by user")
        finally:
            progressBar.close()
            self.saveCheckpoint(globalStep, isFinal=True)
            self.logger.info("üèÅ Training complete!")
    
    def trainingStep(self):
        """Single training step."""
        # Simplified training step
        # In practice, this would implement the actual forward/backward pass
        return 0.1  # Placeholder loss
    
    def evaluate(self):
        """Evaluate the model on validation data."""
        self.logger.info("üìä Running evaluation...")
        # Implement evaluation logic
        pass
    
    def saveCheckpoint(self, step: int, isFinal: bool = False):
        """Save model checkpoint."""
        outputDir = Path(self.config['output']['output_dir'])
        outputDir.mkdir(parents=True, exist_ok=True)
        
        checkpointPath = outputDir / f"checkpoint-{step}"
        # Implement checkpoint saving logic
        
        self.logger.info(f"üíæ Saved checkpoint at {checkpointPath}")

def main():
    parser = argparse.ArgumentParser(description="Train Gemma3 270M with LoRA on MacBook M4")
    parser.add_argument("--config", default="./configs/training_config.json", help="Path to training config")
    parser.add_argument("--data-path", help="Path to training data")
    
    args = parser.parse_args()
    
    # Check if config file exists
    if not Path(args.config).exists():
        print(f"‚ùå Config file not found: {args.config}")
        print("Please run the setup script first: ./setup.sh")
        sys.exit(1)
    
    try:
        # Initialize trainer
        trainer = GemmaLoRATrainer(args.config)
        
        # Override data path if provided
        if args.data_path:
            trainer.config['data']['train_file'] = args.data_path
        
        # Start training
        trainer.train()
        
    except Exception as e:
        print(f"‚ùå Training failed: {e}")
        logging.error(f"Training failed: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
