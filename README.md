# ğŸ§  AI Experiments - AI Playgorund

> A playground for building and testing ideas with AI
> Each project lives in its own folder with all details inside.

## ğŸ“‚ Projects

---

### ğŸ“ **MCPGraph** â€“ W.I.P. 

ğŸ§  A prototype that explores **LLM orchestration using FastMCP** and **graph-based memory via Neo4j**.
It simulates a web search, extracts concepts via prompt chaining, writes them to a concept graph, 
and retrieves contextual answers using graph queries.

ğŸ’¡ The pipeline is fully modular: each step (search, extraction, reranking, graph write/query, summarization) 
is a registered **MCP tool**, allowing for composable and inspectable reasoning.

ğŸ§° Tech used: FastMCP, Neo4j, LangChain, Gemini (`langchain_google_genai`), Python 3.10+

* Full pipeline execution via `fullPipeline(userId, query, preferences)`

---

### ğŸ“ **neo4j** â€“ Hybrid Graph + Embedding RAG Demo

ğŸš€ A proof-of-concept that blends **Neo4j graph relationships** and **vector search** to power a Retrieval-Augmented Generation (RAG) system using **Google Gemini**.
It ingests structured â€œDoctor Whoâ€ data into a `Book â†’ Chapter â†’ Section â†’ Text` graph, builds both **vector** and **graph** indexes, then performs **hybrid search** to answer user questions contextually.

ğŸ§° Tech used: Docker, Python, Neo4j, Sentence-Transformers, LangChain, Gemini
ğŸ“ Scripts:

* `createVectorIndex.py` â€“ Creates a vector index
* `generateEmbeddings.py` â€“ Embeds raw text
* `queryGraphWithEmbedding.py` â€“ Query via embedding
* `createEmbedingsGraphsMetadataRelationships.py` â€“ Full ingestion & indexing
* `hybridVsGraphVsVector.py` â€“ Compare search modes
* `ragAnswerGenerator.py` â€“ RAG via LangChain & Gemini

---

### ğŸ“ **voice-io-lang-graph** â€“ Multi-Agent Voice Chat with LangGraph, Gemini & ElevenLabs

ğŸ—£ï¸ A voice-enabled terminal chatbot where you interact with four chaotic AI personasâ€”**Rick**, **Morty**, **Jerry**, and **Golubiro**, a paranoid drone-pigeon spy. 

Uses **Google Gemini** for natural language and **ElevenLabs** for speech.

ğŸ§  Orchestrated with **LangGraph** as a declarative state machine:

1. You talk (via microphone or type).
2. LangGraph routes your message to the most relevant character using LLM-based classification.
3. The character responds.
4. If the character was Rick or Morty, a follow-up response is triggered by Golubiro or Jerry.
5. The flow loops back to capture the next input.

ğŸ§° Tech used: LangGraph, LangChain, Google Gemini (`langchain_google_genai`), ElevenLabs TTS, `speech_recognition`, Python 3.10+

---
